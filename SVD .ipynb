{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Assignment5-GroupH-SVD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuWZwRjJEqms"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ILZ3Vb5IBe"
      },
      "source": [
        "from __future__ import division\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import scipy.sparse\n",
        "import time\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE as tsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQNZzQ-p-qne"
      },
      "source": [
        "#@title brown corpus reader\n",
        "import sys, os\n",
        "import glob\n",
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "# Struct types for different lines in the bAbI dataset.\n",
        "# StoryLine represents \"ID text\" lines as (int, string)\n",
        "# QALine represents \"ID question answer support\" lines as\n",
        "# (int, string, string, list(int)).\n",
        "# If tokenized, string fields can be replaced with list(string).\n",
        "StoryLine = namedtuple(\"StoryLine\", [\"id\", \"text\"])\n",
        "QALine = namedtuple(\"QALine\", [\"id\", \"question\", \"answer\", \"support_ids\"])\n",
        "\n",
        "class BabiTaskCorpusReader(object):\n",
        "    \"\"\"Corpus reader for the bAbI tasks dataset.\n",
        "\n",
        "    See https://research.fb.com/downloads/babi/ for details.\n",
        "\n",
        "    This class exposes a similar interface to NLTK's corpus readers, and should\n",
        "    be interchangable with them in many applications.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    import babi_utils\n",
        "    import nltk\n",
        "    tok = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
        "    cr = babi_utils.BabiTaskCorpusReader(\"/home/babi/en\",\n",
        "                                         tokenizer=tok.tokenize)\n",
        "    words = list(cr.words())\n",
        "    print words[:8]\n",
        "    # ['John', 'travelled', 'to', 'the', 'hallway', '.', 'Mary', 'journeyed']\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ALL_FILES = [\n",
        "        'qa10_indefinite-knowledge_test.txt',\n",
        "        'qa10_indefinite-knowledge_train.txt',\n",
        "        'qa11_basic-coreference_test.txt',\n",
        "        'qa11_basic-coreference_train.txt',\n",
        "        'qa12_conjunction_test.txt',\n",
        "        'qa12_conjunction_train.txt',\n",
        "        'qa13_compound-coreference_test.txt',\n",
        "        'qa13_compound-coreference_train.txt',\n",
        "        'qa14_time-reasoning_test.txt',\n",
        "        'qa14_time-reasoning_train.txt',\n",
        "        'qa15_basic-deduction_test.txt',\n",
        "        'qa15_basic-deduction_train.txt',\n",
        "        'qa16_basic-induction_test.txt',\n",
        "        'qa16_basic-induction_train.txt',\n",
        "        'qa17_positional-reasoning_test.txt',\n",
        "        'qa17_positional-reasoning_train.txt',\n",
        "        'qa18_size-reasoning_test.txt',\n",
        "        'qa18_size-reasoning_train.txt',\n",
        "        'qa19_path-finding_test.txt',\n",
        "        'qa19_path-finding_train.txt',\n",
        "        'qa1_single-supporting-fact_test.txt',\n",
        "        'qa1_single-supporting-fact_train.txt',\n",
        "        'qa20_agents-motivations_test.txt',\n",
        "        'qa20_agents-motivations_train.txt',\n",
        "        'qa2_two-supporting-facts_test.txt',\n",
        "        'qa2_two-supporting-facts_train.txt',\n",
        "        'qa3_three-supporting-facts_test.txt',\n",
        "        'qa3_three-supporting-facts_train.txt',\n",
        "        'qa4_two-arg-relations_test.txt',\n",
        "        'qa4_two-arg-relations_train.txt',\n",
        "        'qa5_three-arg-relations_test.txt',\n",
        "        'qa5_three-arg-relations_train.txt',\n",
        "        'qa6_yes-no-questions_test.txt',\n",
        "        'qa6_yes-no-questions_train.txt',\n",
        "        'qa7_counting_test.txt',\n",
        "        'qa7_counting_train.txt',\n",
        "        'qa8_lists-sets_test.txt',\n",
        "        'qa8_lists-sets_train.txt',\n",
        "        'qa9_simple-negation_test.txt',\n",
        "        'qa9_simple-negation_train.txt'\n",
        "    ]\n",
        "\n",
        "    def __init__(self, directory, mask=\"qa*.txt\",\n",
        "                 file_list=ALL_FILES,\n",
        "                 file_reader=open,\n",
        "                 tokenizer=lambda s: s.split(),\n",
        "                 verbose=False):\n",
        "        \"\"\"Construct a corpus reader for the bAbI tasks dataset.\n",
        "\n",
        "        Args:\n",
        "            directory: (string) path to bAbI text files (e.g. /home/babi/en/)\n",
        "            mask: (string) file glob to match particular files. Use\n",
        "                \"qa16_*\" e.g. to match task 16.\n",
        "            file_list: (list(string) or None) If None, will glob directory to\n",
        "                find files. Otherwise, will use the given list of basenames.\n",
        "            file_reader: (function string -> fd) optional replacement for\n",
        "                Python's built-in open(...) method, to be used for reading\n",
        "                from alternative file-like objects.\n",
        "            tokenizer: function string -> list(string), used to split\n",
        "                sentences.\n",
        "            verbose: (bool) if true, will print when reading files.\n",
        "        \"\"\"\n",
        "        self._open = file_reader\n",
        "        self._tokenizer = tokenizer\n",
        "        self._verbose = verbose\n",
        "\n",
        "        if file_list:\n",
        "            basenames = glob.fnmatch.filter(file_list, mask)\n",
        "            filenames = [os.path.join(directory, f) for f in basenames]\n",
        "        else:\n",
        "            # Glob directory\n",
        "            pattern = os.path.join(directory, mask)\n",
        "            filenames = glob.glob(pattern)\n",
        "\n",
        "        # Filenames of form qaXX_task-name_train.txt\n",
        "        # Want to sort by XX as a number\n",
        "        key_fn = lambda f: (int(os.path.basename(f).split(\"_\")[0][2:]), f)\n",
        "        self._filenames = sorted(filenames, key=key_fn)\n",
        "        # Filenames should be nonempty!\n",
        "        assert(self._filenames), \"No files found matching [{:s}]\".format(mask)\n",
        "\n",
        "    def filenames(self):\n",
        "        return self._filenames\n",
        "\n",
        "    def parse_line(self, line):\n",
        "        \"\"\"Parse a single line from the bAbI corpus.\n",
        "\n",
        "        Line is of one of the two forms:\n",
        "        ID text\n",
        "        ID question[tab]answer[tab]supporting fact IDs\n",
        "\n",
        "        See https://research.fb.com/downloads/babi/\n",
        "\n",
        "        Args:\n",
        "            line: (string)\n",
        "\n",
        "        Returns:\n",
        "            (id, text) as (int, string)\n",
        "            OR (id, question, answer, [ids]) as (int, string, string, list(int))\n",
        "        \"\"\"\n",
        "        id_text, rest = line.split(\" \", 1)\n",
        "        id = int(id_text)\n",
        "        if \"\\t\" in rest:\n",
        "            question, answer, s_ids_text = rest.split(\"\\t\")\n",
        "            s_ids = map(int, s_ids_text.split())\n",
        "            return QALine(id, question.strip(), answer.strip(), s_ids)\n",
        "        else:\n",
        "            return StoryLine(id, rest.strip())\n",
        "\n",
        "    def tokenize_parsed_line(self, line):\n",
        "        if isinstance(line, StoryLine):\n",
        "            return StoryLine(line.id, self._tokenizer(line.text))\n",
        "        else:\n",
        "            return QALine(line.id,\n",
        "                          self._tokenizer(line.question),\n",
        "                          self._tokenizer(line.answer),\n",
        "                          line.support_ids)\n",
        "\n",
        "    def _line_iterator(self):\n",
        "        for f in self._filenames:\n",
        "            if self._verbose:\n",
        "                print >> sys.stderr, \"Reading {:s}\".format(os.path.basename(f)),\n",
        "            with self._open(f) as fd:\n",
        "                for line in fd:\n",
        "                    yield line.strip()\n",
        "            if self._verbose:\n",
        "                print >> sys.stderr, \"...done!\"\n",
        "\n",
        "    def examples(self, tokenize=True):\n",
        "        \"\"\"Iterator over complete stories (training examples).\n",
        "\n",
        "        A story spans multiple lines, of the form:\n",
        "\n",
        "        1 text one\n",
        "        2 text two\n",
        "        3 text three\n",
        "        4 question[tab]answer[tab]supporting fact IDs\n",
        "\n",
        "        Args:\n",
        "            tokenize: (bool) If true, will tokenize text fields.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding list(StoryLine|QALine)\n",
        "              if tokenize=True, then text, question, and answer will be\n",
        "              list(string); otherwise they will be plain strings.\n",
        "        \"\"\"\n",
        "        buffer = []\n",
        "        for line in self._line_iterator():\n",
        "            parsed = self.parse_line(line)\n",
        "            if tokenize:\n",
        "                parsed = self.tokenize_parsed_line(parsed)\n",
        "            # If new story item, flush buffer.\n",
        "            if buffer and parsed.id <= buffer[-1].id:\n",
        "                yield buffer\n",
        "                buffer = []\n",
        "            buffer.append(parsed)\n",
        "        # Flush at end.\n",
        "        yield buffer\n",
        "        buffer = []\n",
        "\n",
        "    def _raw_sents_impl(self, stories=False, questions=False, answers=False):\n",
        "        for line in self._line_iterator():\n",
        "            parsed = self.parse_line(line)\n",
        "            if isinstance(parsed, StoryLine) and stories:\n",
        "                yield parsed.text\n",
        "            else:\n",
        "                if questions:\n",
        "                    yield parsed.question\n",
        "                if answers:\n",
        "                    yield parsed.answer\n",
        "\n",
        "    def raw_sents(self):\n",
        "        \"\"\"Iterator over utterances in the corpus.\n",
        "\n",
        "        Returns untokenized sentences.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding string\n",
        "        \"\"\"\n",
        "        return self._raw_sents_impl(stories=True,\n",
        "                                    questions=True,\n",
        "                                    answers=True)\n",
        "\n",
        "    def sents(self):\n",
        "        \"\"\"Iterator over utterances in the corpus.\n",
        "\n",
        "        Returns tokenized sentences, a la NLTK.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding list(string)\n",
        "        \"\"\"\n",
        "        for sentence in self.raw_sents():\n",
        "            yield self._tokenizer(sentence)\n",
        "\n",
        "\n",
        "    def words(self):\n",
        "        \"\"\"Iterator over words in the corpus.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding string\n",
        "        \"\"\"\n",
        "        for sentence in self.sents():\n",
        "            for word in sentence:\n",
        "                yield word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO3lI3xhVA54"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "local_tar = '/tmp/babi_tasks_1-20_v1-2.tar.gz'\n",
        "tar_ref = tarfile.open(local_tar, 'r:gz')\n",
        "tar_ref.extractall('/tmp')\n",
        "tar_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE4WxeIKT1wN",
        "outputId": "6e2665d8-004d-4861-f597-25349376ba03"
      },
      "source": [
        "# Load the Drive helper and mount google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmIJ3Xdt5cPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af5f425-9131-40c1-df1d-0cd7b26430eb"
      },
      "source": [
        "print 'Loading bAbI corpus... ',\n",
        "import operator, functools\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "babi_corpus = brown\n",
        "print 'Done'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading bAbI corpus... [nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            " Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2dt5cEBDWRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67829089-305e-497a-c046-64eaaf5063fb"
      },
      "source": [
        "corpus = list(babi_corpus.sents())\n",
        "print 'Sentences:', len(corpus)\n",
        "\n",
        "# Print the first 5 sentences of the corpus.\n",
        "for i, sent in enumerate(corpus[:5]):\n",
        "    print i, ' '.join(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences: 57340\n",
            "0 The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "1 The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
            "2 The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
            "3 `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "4 The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_iG4zgkFE6s"
      },
      "source": [
        "#@title Utilities\n",
        "import re\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# For pretty-printing\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "UNK_TOKEN   = u\"<unk>\"\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
        "    return list(itertools.chain.from_iterable(list_of_lists))\n",
        "\n",
        "def pretty_print_matrix(M, rows=None, cols=None, dtype=float, float_fmt=\"{0:.04f}\"):\n",
        "    \"\"\"Pretty-print a matrix using Pandas.\n",
        "\n",
        "    Args:\n",
        "      M : 2D numpy array\n",
        "      rows : list of row labels\n",
        "      cols : list of column labels\n",
        "      dtype : data type (float or int)\n",
        "      float_fmt : format specifier for floats\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(M, index=rows, columns=cols, dtype=dtype)\n",
        "    old_fmt_fn = pd.get_option('float_format')\n",
        "    pd.set_option('float_format', lambda f: float_fmt.format(f))\n",
        "    display(df)\n",
        "    pd.set_option('float_format', old_fmt_fn)  # reset Pandas formatting\n",
        "\n",
        "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
        "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
        "    since = since or time.time()\n",
        "    until = until or time.time()\n",
        "    delta_s = until - since\n",
        "    hours, remainder = divmod(delta_s, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    return fmt % (hours, minutes, seconds)\n",
        "\n",
        "\n",
        "##\n",
        "# Word processing functions\n",
        "def canonicalize_digits(word):\n",
        "    if any([c.isalpha() for c in word]): return word\n",
        "    word = re.sub(\"\\d\", \"DG\", word)\n",
        "    if word.startswith(\"DG\"):\n",
        "        word = word.replace(\",\", \"\") # remove thousands separator\n",
        "    return word\n",
        "\n",
        "def canonicalize_word(word, wordset=None, digits=True):\n",
        "    word = word.lower()\n",
        "    if digits:\n",
        "        if (wordset != None) and (word in wordset): return word\n",
        "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
        "    if (wordset == None) or (word in wordset):\n",
        "        return word\n",
        "    else:\n",
        "        return UNK_TOKEN\n",
        "\n",
        "def canonicalize_words(words, **kw):\n",
        "    return [canonicalize_word(word, **kw) for word in words]\n",
        "\n",
        "##\n",
        "# Data loading functions\n",
        "def get_corpus(name=\"brown\"):\n",
        "    import nltk\n",
        "    assert(nltk.download(name))\n",
        "    return nltk.corpus.__getattr__(name)\n",
        "\n",
        "def build_vocab(corpus, V=10000):\n",
        "    import vocabulary\n",
        "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
        "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
        "    return vocab\n",
        "\n",
        "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
        "    \"\"\"Generate train/test split for unsupervised tasks.\n",
        "\n",
        "    Args:\n",
        "      corpus: nltk.corpus that supports sents() function\n",
        "      split (double): fraction to use as training set\n",
        "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
        "      take the training data as the first xx% contiguously.\n",
        "\n",
        "    Returns:\n",
        "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
        "      splits\n",
        "    \"\"\"\n",
        "    sentences = np.array(list(corpus.sents()), dtype=object)\n",
        "    fmt = (len(sentences), sum(map(len, sentences)))\n",
        "    print \"Loaded {:,} sentences ({:g} tokens)\".format(*fmt)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = np.random.RandomState(shuffle)\n",
        "        rng.shuffle(sentences)  # in-place\n",
        "    train_frac = 0.8\n",
        "    split_idx = int(train_frac * len(sentences))\n",
        "    train_sentences = sentences[:split_idx]\n",
        "    test_sentences = sentences[split_idx:]\n",
        "\n",
        "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
        "    print \"Training set: {:,} sentences ({:,} tokens)\".format(*fmt)\n",
        "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
        "    print \"Test set: {:,} sentences ({:,} tokens)\".format(*fmt)\n",
        "\n",
        "    return train_sentences, test_sentences\n",
        "\n",
        "def preprocess_sentences(sentences, vocab, use_eos=False, emit_ids=True):\n",
        "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
        "\n",
        "    Args:\n",
        "      sentences ( list(list(string)) ): input sentences\n",
        "      vocab: Vocabulary object, already initialized\n",
        "      use_eos: if true, will add </s> token to end of sentence.\n",
        "      emit_ids: if true, will emit as ids. Otherwise, will be preprocessed\n",
        "          tokens.\n",
        "\n",
        "    Returns:\n",
        "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
        "      tokens.\n",
        "    \"\"\"\n",
        "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
        "    word_preproc = lambda w: canonicalize_word(w, wordset=vocab.word_to_id)\n",
        "    ret = []\n",
        "    for s in sentences:\n",
        "        canonical_words = vocab.pad_sentence(map(word_preproc, s),\n",
        "                                             use_eos=use_eos)\n",
        "        ret.extend(vocab.words_to_ids(canonical_words) if emit_ids else\n",
        "                   canonical_words)\n",
        "    if not use_eos:  # add additional <s> to end if needed\n",
        "        ret.append(vocab.START_ID if emit_ids else vocab.START_TOKEN)\n",
        "    return np.array(ret, dtype=(np.int32 if emit_ids else object))\n",
        "\n",
        "\n",
        "def load_corpus(corpus, split=0.8, V=10000, shuffle=0):\n",
        "    \"\"\"Load a named corpus and split train/test along sentences.\n",
        "\n",
        "    This is a convenience wrapper to chain together several functions from this\n",
        "    module, and produce a train/test split suitable for input to most models.\n",
        "\n",
        "    Sentences are preprocessed by canonicalization and converted to ids\n",
        "    according to the constructed vocabulary, and interspersed with <s> tokens\n",
        "    to denote sentence bounaries.\n",
        "\n",
        "    Args:\n",
        "        corpus: (string | corpus reader) If a string, will fetch the\n",
        "            NLTK corpus of that name.\n",
        "        split: (float \\in (0,1]) fraction of examples in train split\n",
        "        V: (int) vocabulary size (including special tokens)\n",
        "        shuffle: (int) if > 0, use as random seed to shuffle sentence prior to\n",
        "            split. Can change this to get different splits.\n",
        "\n",
        "    Returns:\n",
        "        (vocab, train_ids, test_ids)\n",
        "        vocab: vocabulary.Vocabulary object\n",
        "        train_ids: flat (1D) np.array(int) of ids\n",
        "        test_ids: flat (1D) np.array(int) of ids\n",
        "    \"\"\"\n",
        "    if isinstance(corpus, str):\n",
        "        corpus = get_corpus(corpus)\n",
        "    vocab = build_vocab(corpus, V)\n",
        "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
        "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
        "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
        "    return vocab, train_ids, test_ids\n",
        "\n",
        "##\n",
        "# Window and batch functions\n",
        "def rnnlm_batch_generator(ids, batch_size, max_time):\n",
        "    \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
        "    # Clip to multiple of max_time for convenience\n",
        "    clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
        "    input_w = ids[:clip_len]     # current word\n",
        "    target_y = ids[1:clip_len+1]  # next word\n",
        "    # Reshape so we can select columns\n",
        "    input_w = input_w.reshape([batch_size,-1])\n",
        "    target_y = target_y.reshape([batch_size,-1])\n",
        "\n",
        "    # Yield batches\n",
        "    for i in xrange(0, input_w.shape[1], max_time):\n",
        "        yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
        "\n",
        "\n",
        "def build_windows(ids, N, shuffle=True):\n",
        "    \"\"\"Build window input to the window model.\n",
        "\n",
        "    Takes a sequence of ids, and returns a data matrix where each row\n",
        "    is a window and target for the window model. For N=3:\n",
        "        windows[i] = [w_3, w_2, w_1, w_0]\n",
        "\n",
        "    For language modeling, N is the context size and you can use y = windows[:,-1]\n",
        "    as the target words and x = windows[:,:-1] as the contexts.\n",
        "\n",
        "    For CBOW, N is the window size and you can use y = windows[:,N/2] as the target words\n",
        "    and x = np.hstack([windows[:,:N/2], windows[:,:N/2+1]]) as the contexts.\n",
        "\n",
        "    For skip-gram, you can use x = windows[:,N/2] as the input words and y = windows[:,i]\n",
        "    where i != N/2 as the target words.\n",
        "\n",
        "    Args:\n",
        "      ids: np.array(int32) of input ids\n",
        "      shuffle: if true, will randomly shuffle the rows\n",
        "\n",
        "    Returns:\n",
        "      windows: np.array(int32) of shape [len(ids)-N, N+1]\n",
        "        i.e. each row is a window, of length N+1\n",
        "    \"\"\"\n",
        "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
        "    for i in xrange(N+1):\n",
        "        # First column: first word, etc.\n",
        "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
        "    if shuffle:\n",
        "        # Shuffle rows\n",
        "        np.random.shuffle(windows)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Generate minibatches from data.\n",
        "\n",
        "    Args:\n",
        "      data: array-like, supporting slicing along first dimension\n",
        "      batch_size: int, batch size\n",
        "\n",
        "    Yields:\n",
        "      minibatches of maximum size batch_size\n",
        "    \"\"\"\n",
        "    for i in xrange(0, len(data), batch_size):\n",
        "        yield data[i:i+batch_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nuZcrMnHKRD",
        "cellView": "form"
      },
      "source": [
        "#@title Vocabulary helper functions\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "  START_TOKEN = u\"<s>\"\n",
        "  END_TOKEN   = u\"</s>\"\n",
        "  UNK_TOKEN   = u\"<unk>\"\n",
        "\n",
        "  def __init__(self, tokens, size=None):\n",
        "    \"\"\"Create a Vocabulary object.\n",
        "\n",
        "    Args:\n",
        "        tokens: iterator( string )\n",
        "        size: None for unlimited, or int > 0 for a fixed-size vocab.\n",
        "              Vocabulary size includes special tokens <s>, </s>, and <unk>\n",
        "    \"\"\"\n",
        "    self.unigram_counts = collections.Counter(tokens)\n",
        "    self.bigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    word1 = None\n",
        "    for word in tokens:\n",
        "        if word1 is None:\n",
        "            pass\n",
        "        self.bigram_counts[word1][word] += 1\n",
        "        word1 = word\n",
        "    self.bigram_counts.default_factory = None  # make into a normal dict\n",
        "\n",
        "    # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
        "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
        "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
        "             [w for w,c in top_counts])\n",
        "\n",
        "    # Assign an id to each word, by frequency\n",
        "    self.id_to_word = dict(enumerate(vocab))\n",
        "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
        "    self.size = len(self.id_to_word)\n",
        "    if size is not None:\n",
        "        assert(self.size <= size)\n",
        "\n",
        "    # For convenience\n",
        "    self.wordset = set(self.word_to_id.iterkeys())\n",
        "\n",
        "    # Store special IDs\n",
        "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
        "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
        "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
        "\n",
        "  def words_to_ids(self, words):\n",
        "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
        "\n",
        "  def ids_to_words(self, ids):\n",
        "    return [self.id_to_word[i] for i in ids]\n",
        "\n",
        "  def pad_sentence(self, words, use_eos=True):\n",
        "    ret = [self.START_TOKEN] + words\n",
        "    if use_eos:\n",
        "      ret.append(self.END_TOKEN)\n",
        "    return ret\n",
        "\n",
        "  def sentence_to_ids(self, words, use_eos=True):\n",
        "    return self.words_to_ids(self.pad_sentence(words, use_eos))\n",
        "\n",
        "  def ordered_words(self):\n",
        "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
        "    return self.ids_to_words(range(self.size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOPBVbRHjIO"
      },
      "source": [
        "#@title Corpus Reader\n",
        "import sys, os\n",
        "\n",
        "class TSVCorpusReader(object):\n",
        "    \"\"\"Corpus reader for TSV files.\n",
        "\n",
        "    Input files are assumed to contain one sentence per line, with tokens\n",
        "    separated by tabs:\n",
        "\n",
        "    foo[tab]bar[tab]baz\n",
        "    span[tab]eggs\n",
        "\n",
        "    Would correspond to the two-sentence corpus:\n",
        "        [\"foo\", \"bar\", \"baz\"],\n",
        "        [\"spam\", \"eggs\"]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence_file, preload=True, file_reader=open):\n",
        "        \"\"\"Construct a corpus reader for the given file.\n",
        "\n",
        "        Args:\n",
        "            sentence_file: (string) path to a TSV file with one sentence per\n",
        "                line.\n",
        "            preload: (bool) If true, will read entire corpus to memory on\n",
        "                construction. Otherwise, will load on-demand.\n",
        "            file_reader: (function string -> fd) optional replacement for\n",
        "                Python's built-in open(...) method, to be used for reading\n",
        "                from alternative file-like objects.\n",
        "        \"\"\"\n",
        "        self._open = file_reader\n",
        "        self._sentence_file = sentence_file\n",
        "        self._sentence_cache = []\n",
        "\n",
        "        if preload:\n",
        "            self._sentence_cache = list(self.sents())\n",
        "\n",
        "    def _line_iterator(self):\n",
        "        with self._open(self._sentence_file) as fd:\n",
        "            for line in fd:\n",
        "                yield line.strip()\n",
        "\n",
        "    def sents(self):\n",
        "        \"\"\"Iterator over sentences in the corpus.\n",
        "\n",
        "        Yields:\n",
        "            list(string) of tokens\n",
        "        \"\"\"\n",
        "        if self._sentence_cache:\n",
        "            for sentence in self._sentence_cache:\n",
        "                yield sentence\n",
        "        else:\n",
        "            # If no cache, actually read the file.\n",
        "            for line in self._line_iterator():\n",
        "                yield line.split(\"\\t\")\n",
        "\n",
        "    def words(self):\n",
        "        \"\"\"Iterator over words in the corpus.\n",
        "\n",
        "        Yields:\n",
        "            (string) tokens\n",
        "        \"\"\"\n",
        "        for sentence in self.sents():\n",
        "            for word in sentence:\n",
        "                yield word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvSkjdiOHzUk"
      },
      "source": [
        "## Creating the vocabulary\n",
        "\n",
        "Let's now get started with creating the vocabulary. We'll use some of the functions defined in the utility classes we just loaded above.\n",
        "\n",
        "(Note: the following code cell may take 20-30 seconds to complete running.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W8Bbx_BENcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf15ffe6-73a0-4c94-fe29-604eb4023780"
      },
      "source": [
        "# Create a vocabulary by first canonicalizing all the words -- lowercasing\n",
        "# and converting all digits to a single string. The vocabulary maintains a\n",
        "# mapping between words and integer ids.\n",
        "vocab = Vocabulary(canonicalize_word(w)\n",
        "                   for w in flatten(corpus))\n",
        "print \"Vocabulary: {:,} words\".format(vocab.size)\n",
        "\n",
        "# Turn the corpus into a single flattened list of tokens, where each sentence\n",
        "# begins with a special marker <s>.\n",
        "tokens = preprocess_sentences(corpus, vocab, use_eos=False, emit_ids=False)\n",
        "print \"Corpus: {:,} tokens (counting <s>)\".format(len(tokens))\n",
        "\n",
        "# Retrieve the ids corresponding to the tokens (above). This is the data\n",
        "# we'll actually use.\n",
        "token_ids = vocab.words_to_ids(tokens)\n",
        "print 'Sample words:', tokens[:10]\n",
        "print 'Sample ids:', token_ids[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: 48,174 words\n",
            "Corpus: 1,218,533 tokens (counting <s>)\n",
            "Sample words: [u'<s>' u'the' u'fulton' u'county' u'grand' u'jury' u'said' u'friday'\n",
            " u'an' u'investigation']\n",
            "Sample ids: [0, 3, 5613, 655, 2288]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPySe-BBEVRy"
      },
      "source": [
        "# A function that produces a sparse co-occurrence matrix given a corpus,\n",
        "# a vocabulary size V, and K (the context window is +-K).\n",
        "\n",
        "from scipy import *\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def co_occurrence_matrix(token_ids, V, K=2):\n",
        "    # We'll use this as an \"accumulator\" matrix.\n",
        "    C = csc_matrix((V,V), dtype=np.float32)\n",
        "\n",
        "    for k in range(1, K+1):\n",
        "        print (u'Counting pairs (i, i \\u00B1 %d) ...' %k)\n",
        "        i = token_ids[:-k]  # current word\n",
        "        j = token_ids[k:]   # k words ahead\n",
        "        data = (np.ones_like(i), (i,j))  # values, indices\n",
        "        Ck_plus = coo_matrix(data, shape=C.shape, dtype=np.float32)\n",
        "        Ck_plus = scipy.sparse.csc_matrix(Ck_plus)\n",
        "        Ck_minus = Ck_plus.T  # consider k words behind\n",
        "        C += Ck_plus + Ck_minus\n",
        "\n",
        "    print( \"Co-occurrence matrix: %d words x %d words\" %C.shape)\n",
        "    print (\"  %.02g nonzero elements\" %C.nnz)\n",
        "    return C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcdoXTrKEZjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "37bf905e-24ce-46f5-aac4-bc9a7338b479"
      },
      "source": [
        "# Build a toy corpus with the same shape as our corpus object.\n",
        "toy_corpus = [\n",
        "    \"nlp class is awesome\",\n",
        "    \"nlp is awesome fun\"\n",
        "]\n",
        "toy_corpus = map(str.split, toy_corpus)\n",
        "\n",
        "# Get vocab, tokens, and token_ids as above.\n",
        "toy_vocab = Vocabulary(canonicalize_word(w)\n",
        "                       for w in flatten(toy_corpus))\n",
        "toy_tokens = preprocess_sentences(toy_corpus, toy_vocab,\n",
        "                                  use_eos=False, emit_ids=False)\n",
        "toy_token_ids = toy_vocab.words_to_ids(toy_tokens)\n",
        "\n",
        "# Build the co-occurrence matrix.\n",
        "toy_C = co_occurrence_matrix(toy_token_ids, toy_vocab.size, K=1)\n",
        "\n",
        "# Display a table with the counts. The .toarray() function converts the\n",
        "# sparse matrix into a dense one.\n",
        "toy_labels = toy_vocab.ordered_words()\n",
        "pretty_print_matrix(toy_C.toarray(), rows=toy_labels,\n",
        "                    cols=toy_labels, dtype=int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting pairs (i, i ± 1) ...\n",
            "Co-occurrence matrix: 8 words x 8 words\n",
            "  16 nonzero elements\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         <s>  </s>  <unk>  nlp  is  awesome  fun  class\n",
              "<s>        0     0      0    2   0        1    1      0\n",
              "</s>       0     0      0    0   0        0    0      0\n",
              "<unk>      0     0      0    0   0        0    0      0\n",
              "nlp        2     0      0    0   1        0    0      1\n",
              "is         0     0      0    1   0        2    0      1\n",
              "awesome    1     0      0    0   2        0    1      0\n",
              "fun        1     0      0    0   0        1    0      0\n",
              "class      0     0      0    1   1        0    0      0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <th>&lt;/s&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <th>nlp</th>\n",
              "      <th>is</th>\n",
              "      <th>awesome</th>\n",
              "      <th>fun</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;/s&gt;</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nlp</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>awesome</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fun</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps1SDsoYEhua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "f98ebc50-7189-4777-f4c6-cc143addc259"
      },
      "source": [
        "def PPMI(C):\n",
        "    \"\"\"Tranform a counts matrix to PPMI.\n",
        "    \n",
        "    Args:\n",
        "      C: scipy.sparse.csc_matrix of counts C_ij\n",
        "    \n",
        "    Returns:\n",
        "      (scipy.sparse.csc_matrix) PPMI(C) as defined above\n",
        "    \"\"\"\n",
        "    # Total count.\n",
        "    Z = float(C.sum())\n",
        "\n",
        "    # Sum each row (along columns).\n",
        "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten()\n",
        "    \n",
        "    # Get indices of relevant elements.\n",
        "    ii, jj = C.nonzero()  # row, column indices\n",
        "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
        "    \n",
        "    # PMI equation.\n",
        "    pmi = np.log(Cij * Z / (Zr[ii] * Zr[jj]))\n",
        "\n",
        "    # Truncate to positive only.\n",
        "    ppmi = np.maximum(0, pmi)  # take positive only\n",
        "    \n",
        "    # Re-format as sparse matrix.\n",
        "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
        "                                  dtype=np.float64)\n",
        "    ret.eliminate_zeros()  # remove zeros\n",
        "    return ret\n",
        "\n",
        "# Display the PPMI'd version of the co-occurrence matrix.\n",
        "pretty_print_matrix(PPMI(toy_C).toarray(), rows=toy_labels, \n",
        "                    cols=toy_labels, dtype=float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           <s>   </s>  <unk>    nlp     is  awesome    fun  class\n",
              "<s>     0.0000 0.0000 0.0000 0.9163 0.0000   0.2231 0.9163 0.0000\n",
              "</s>    0.0000 0.0000 0.0000 0.0000 0.0000   0.0000 0.0000 0.0000\n",
              "<unk>   0.0000 0.0000 0.0000 0.0000 0.0000   0.0000 0.0000 0.0000\n",
              "nlp     0.9163 0.0000 0.0000 0.0000 0.2231   0.0000 0.0000 0.9163\n",
              "is      0.0000 0.0000 0.0000 0.2231 0.0000   0.9163 0.0000 0.9163\n",
              "awesome 0.2231 0.0000 0.0000 0.0000 0.9163   0.0000 0.9163 0.0000\n",
              "fun     0.9163 0.0000 0.0000 0.0000 0.0000   0.9163 0.0000 0.0000\n",
              "class   0.0000 0.0000 0.0000 0.9163 0.9163   0.0000 0.0000 0.0000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <th>&lt;/s&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <th>nlp</th>\n",
              "      <th>is</th>\n",
              "      <th>awesome</th>\n",
              "      <th>fun</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2231</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;/s&gt;</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nlp</th>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2231</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2231</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>awesome</th>\n",
              "      <td>0.2231</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fun</th>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df_kpOsyEnf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "76e950db-5993-43b7-f11d-31f324d75daa"
      },
      "source": [
        "def SVD(X, d=2):\n",
        "    \"\"\"Returns word vectors from SVD.\n",
        "    \n",
        "    Args:\n",
        "      X: [m, n] matrix\n",
        "      d: word vector dimension\n",
        "      \n",
        "    Returns:\n",
        "      Wv : [m, d] matrix, where each row is a word vector.\n",
        "    \"\"\"\n",
        "    transformer = TruncatedSVD(n_components=d, random_state=0)\n",
        "    Wv = transformer.fit_transform(X)\n",
        "    \n",
        "    # Normalize all vectors to unit length.\n",
        "    Wv = Wv / np.linalg.norm(Wv, axis=1).reshape([-1,1])\n",
        "    \n",
        "    print 'Computed embeddings:', Wv.shape\n",
        "    return Wv\n",
        "\n",
        "# Compute 3-dimensional word embeddings for the toy corpus.\n",
        "dim = 3\n",
        "embeddings = SVD(PPMI(toy_C).toarray(), d=dim)\n",
        "pretty_print_matrix(embeddings, rows=toy_labels, cols=range(dim), dtype=float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (8, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0       1       2\n",
              "<s>      0.6871 -0.5533  0.4709\n",
              "</s>     0.0785 -0.1472  0.9860\n",
              "<unk>   -0.0505  0.1088  0.9928\n",
              "nlp      0.6871  0.5533 -0.4709\n",
              "is       0.6871  0.5533  0.4709\n",
              "awesome  0.6871 -0.5533 -0.4709\n",
              "fun      0.7260  0.6877  0.0000\n",
              "class    0.7260 -0.6877 -0.0000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <td>0.6871</td>\n",
              "      <td>-0.5533</td>\n",
              "      <td>0.4709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;/s&gt;</th>\n",
              "      <td>0.0785</td>\n",
              "      <td>-0.1472</td>\n",
              "      <td>0.9860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <td>-0.0505</td>\n",
              "      <td>0.1088</td>\n",
              "      <td>0.9928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nlp</th>\n",
              "      <td>0.6871</td>\n",
              "      <td>0.5533</td>\n",
              "      <td>-0.4709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>0.6871</td>\n",
              "      <td>0.5533</td>\n",
              "      <td>0.4709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>awesome</th>\n",
              "      <td>0.6871</td>\n",
              "      <td>-0.5533</td>\n",
              "      <td>-0.4709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fun</th>\n",
              "      <td>0.7260</td>\n",
              "      <td>0.6877</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0.7260</td>\n",
              "      <td>-0.6877</td>\n",
              "      <td>-0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnruGindZZeo"
      },
      "source": [
        "Now let's try computing word vectors on the large corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwLqHK1LEqFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00d7b27-60ff-40db-e70c-b23c48e0e71b"
      },
      "source": [
        "# Compute 100-dimensional embeddings with a window of size 2.\n",
        "C = PPMI(co_occurrence_matrix(token_ids, vocab.size, K=2))\n",
        "embeddings = SVD(C, d=100)\n",
        "word_list = vocab.ids_to_words(range(48174))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting pairs (i, i ± 1) ...\n",
            "Counting pairs (i, i ± 2) ...\n",
            "Co-occurrence matrix: 48174 words x 48174 words\n",
            "  1.5e+06 nonzero elements\n",
            "Computed embeddings: (48174, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDAvYp0SJkjv",
        "outputId": "05945f49-eaaf-4c61-df0f-d90462cf650d"
      },
      "source": [
        "# Compute 300-dimensional embeddings with a window of size 2.\r\n",
        "embeddings2300 = SVD(C, d=300)\r\n",
        "embeddings2300"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.14057577e-01,  2.76499605e-01,  2.63717840e-01, ...,\n",
              "        -8.20463560e-03,  2.32135146e-02,  6.25396034e-03],\n",
              "       [ 4.65172740e-06,  6.01838270e-04, -3.23303182e-03, ...,\n",
              "         4.15323461e-03, -2.75751686e-02,  1.35739778e-02],\n",
              "       [-9.23847547e-07, -3.26437064e-04,  2.77146934e-04, ...,\n",
              "         2.23685013e-02, -3.05506652e-02,  7.14481164e-02],\n",
              "       ...,\n",
              "       [ 1.26243448e-01, -1.64350582e-01,  1.15027950e-01, ...,\n",
              "        -7.36444925e-03, -5.37011144e-04,  1.55631558e-03],\n",
              "       [ 7.82907828e-02, -6.54041587e-02, -9.23114304e-03, ...,\n",
              "         2.42974959e-02, -7.63937485e-02,  1.01232290e-02],\n",
              "       [ 1.62967932e-01, -2.30945697e-01,  3.57959357e-01, ...,\n",
              "        -3.63380710e-03,  4.77436752e-02,  7.66685009e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FYDpKml6TKk"
      },
      "source": [
        "def writedata(filename , X):\r\n",
        "  np.savetxt(filename, X)\r\n",
        "  with open(filename, 'r' ) as file:\r\n",
        "    data= file.readlines()\r\n",
        "  count=0\r\n",
        "  while count < 48174:\r\n",
        "    data[count]= word_list[count] + \" \"+ data[count]\r\n",
        "    count=count+1\r\n",
        "  with open(filename, 'w') as file:\r\n",
        "    file.writelines(data)\r\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOYECIVaJ7U_"
      },
      "source": [
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/f_SVD_2_300.txt\", embeddings2300)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmr4kcRfNr2H",
        "outputId": "2d4124b5-1f37-4d10-dad3-86433dd07722"
      },
      "source": [
        "embeddings21000 = SVD(C, d=1000)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_2_1000.txt\", embeddings21000)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmiQqp1YPd6p",
        "outputId": "b0dfcea7-8717-4ba1-e3fd-e62e73e7c0be"
      },
      "source": [
        "# Compute 100-dimensional embeddings with a window of size 10.\r\n",
        "C = PPMI(co_occurrence_matrix(token_ids, vocab.size, K=10))\r\n",
        "embeddings10100 = SVD(C, d=100)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting pairs (i, i ± 1) ...\n",
            "Counting pairs (i, i ± 2) ...\n",
            "Counting pairs (i, i ± 3) ...\n",
            "Counting pairs (i, i ± 4) ...\n",
            "Counting pairs (i, i ± 5) ...\n",
            "Counting pairs (i, i ± 6) ...\n",
            "Counting pairs (i, i ± 7) ...\n",
            "Counting pairs (i, i ± 8) ...\n",
            "Counting pairs (i, i ± 9) ...\n",
            "Counting pairs (i, i ± 10) ...\n",
            "Co-occurrence matrix: 48174 words x 48174 words\n",
            "  5.9e+06 nonzero elements\n",
            "Computed embeddings: (48174, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwfXR-pEP5yq"
      },
      "source": [
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_10_100.txt\", embeddings10100)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD7BMJ8JQMI_",
        "outputId": "aa5b6540-0f4a-46b9-c091-e65745cb8ddb"
      },
      "source": [
        "embeddings10300 = SVD(C, d=300)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_10_300.txt\", embeddings10300)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9gWMicKWyql",
        "outputId": "0989fb31-0018-48f1-b6c9-b59b2b20ef0b"
      },
      "source": [
        "embeddings101000 = SVD(C, d=1000)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_10_1000.txt\", embeddings101000)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rirs2qlyXh5w",
        "outputId": "cda1242c-63bb-4bfb-a691-d23f523db10c"
      },
      "source": [
        "# Compute 100-dimensional embeddings with a window of size 5.\r\n",
        "C = PPMI(co_occurrence_matrix(token_ids, vocab.size, K=5))\r\n",
        "embeddings5100 = SVD(C, d=100)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_5_100.txt\", embeddings5100)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting pairs (i, i ± 1) ...\n",
            "Counting pairs (i, i ± 2) ...\n",
            "Counting pairs (i, i ± 3) ...\n",
            "Counting pairs (i, i ± 4) ...\n",
            "Counting pairs (i, i ± 5) ...\n",
            "Co-occurrence matrix: 48174 words x 48174 words\n",
            "  3.4e+06 nonzero elements\n",
            "Computed embeddings: (48174, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsldShgWZd77",
        "outputId": "2e535283-e8f2-491d-a3fa-da2d9721e1c1"
      },
      "source": [
        "embeddings5300 = SVD(C, d=300)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_5_300.txt\", embeddings5300)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z8zdl7TZ720",
        "outputId": "c49017f0-3b80-42d8-e88f-e6dd5ae15ce3"
      },
      "source": [
        "embeddings51000 = SVD(C, d=1000)\r\n",
        "writedata(\"/content/drive/My Drive/Colab Notebooks/data/SVD_5_1000.txt\", embeddings51000)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computed embeddings: (48174, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}