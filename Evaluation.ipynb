{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Assignment5-GroupH-Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPkkKSBsgAOR",
        "outputId": "eeddba23-e1a6-469b-f3a0-b5c07d07ae85"
      },
      "source": [
        "# Load the Drive helper and mount google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4uDryvDiThE"
      },
      "source": [
        "import os \r\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdAgApLfeg95",
        "outputId": "f34d4c84-01f4-4a84-abc7-025f220c20ea"
      },
      "source": [
        "'''\r\n",
        "Embedding evaluation script for COMS W4705 Fall 2019 HW3.\r\n",
        "\r\n",
        "You can run this script from command-line on one file at a time using\r\n",
        "\r\n",
        "    $ python evaluate.py (name-of-file-containing-trained-model) ;\r\n",
        "\r\n",
        "or you can run it on multiple models at a time by importing the\r\n",
        "evaluate_models() function from it and passing that a list of files.\r\n",
        "\r\n",
        "Accepted file formats include gensim KeyedVectors files (i.e. some filepath\r\n",
        "after you have trained a Word2Vec model and called model.wv.save(filepath)),\r\n",
        "or .txt files with one embedding per line, where each line contains the word\r\n",
        "and then each index of its corresponding embedding, whitespace-separated (i.e.\r\n",
        "there should be something like\r\n",
        "\r\n",
        "    apple 1.0 3.5 2.2 0.9 3.7\r\n",
        "\r\n",
        "on each line).\r\n",
        "\r\n",
        "Warning: data file paths are hardcoded in, so don't modify paths in this file\r\n",
        "or move any of the data files in the directory it resides in!\r\n",
        "'''\r\n",
        "\r\n",
        "import argparse\r\n",
        "import numpy as np\r\n",
        "from scipy.stats import spearmanr\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from scipy.spatial.distance import cosine\r\n",
        "from numpy.linalg import norm\r\n",
        "\r\n",
        "import random\r\n",
        "import os\r\n",
        "'''\r\n",
        "Data processing helper functions.\r\n",
        "'''\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import brown\r\n",
        "from nltk.tokenize import casual\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.test.utils import datapath\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def load_msr(f, limit=None):\r\n",
        "    '''\r\n",
        "        Loads the MSR paraphrase corpus.\r\n",
        "    '''\r\n",
        "    lines = [x.strip().lower().split('\\t') for x in open(f, 'r', encoding='utf-8').readlines()[1:]]\r\n",
        "    sents = [[x[3].split(), x[4].split()] for x in lines]\r\n",
        "    labels = [int(x[0]) for x in lines]\r\n",
        "    return sents, labels\r\n",
        "\r\n",
        "def load_w2v(f):\r\n",
        "    '''\r\n",
        "        A wrapper for loading with gensim's KeyedVectors in word2vec format.\r\n",
        "    '''\r\n",
        "    return KeyedVectors.load_word2vec_format(f, binary=f.endswith('.bin'))\r\n",
        "\r\n",
        "def load_kv(f):\r\n",
        "    '''\r\n",
        "        A wrapper for loading with gensim's KeyedVectors.\r\n",
        "    '''\r\n",
        "    return KeyedVectors.load(f)\r\n",
        "\r\n",
        "def load_txt(f):\r\n",
        "    '''\r\n",
        "        Loads vectors from a text file.\r\n",
        "    '''\r\n",
        "    vectors = {}\r\n",
        "    \r\n",
        "    for line in open(f, 'r').readlines():\r\n",
        "        splits = line.strip().split()\r\n",
        "        vectors[splits[0]] = np.array([float(x) for x in splits[1:]])\r\n",
        "\r\n",
        "    return vectors\r\n",
        "\r\n",
        "def load_model(f):\r\n",
        "    '''\r\n",
        "        Guesses the file format and loads.\r\n",
        "    '''\r\n",
        "    if f.endswith('.bin'): return load_w2v(f)\r\n",
        "    elif f.endswith('.txt'): return load_txt(f)\r\n",
        "    else: return load_kv(f)\r\n",
        "\r\n",
        "def collect(model):\r\n",
        "    '''\r\n",
        "        Collects matrix and vocabulary list from a trained model.\r\n",
        "        Helper function. You shouldn't have to call this yourself.\r\n",
        "    '''\r\n",
        "    if type(model) is dict:\r\n",
        "        vocab = [k for k in model.keys()]\r\n",
        "    else:\r\n",
        "        vocab = [k for k in model.vocab.keys()]\r\n",
        "\r\n",
        "    indices = {}\r\n",
        "    for i in range(len(vocab)): indices[vocab[i]] = i\r\n",
        "        \r\n",
        "    matrix = []\r\n",
        "    for w in vocab:\r\n",
        "        matrix.append(model[w])\r\n",
        "    return np.array(matrix), vocab, indices\r\n",
        "        \r\n",
        "def eval_wordsim(model, f='data/wordsim353/combined.tab'):\r\n",
        "    '''\r\n",
        "        Evaluates a trained embedding model on WordSim353 using cosine\r\n",
        "        similarity and Spearman's rho. Returns a tuple containing\r\n",
        "        (correlation, p-value).\r\n",
        "    '''\r\n",
        "    sim = []\r\n",
        "    pred = []\r\n",
        "\r\n",
        "    for line in open(f, 'r').readlines():\r\n",
        "        splits = line.split('\\t')\r\n",
        "        w1 = splits[0] if splits[0] in model else splits[0].lower()\r\n",
        "        w2 = splits[1] if splits[1] in model else splits[1].lower()\r\n",
        "        if w1 in model and w2 in model:\r\n",
        "            sim.append(float(splits[2]))\r\n",
        "            v1 = model[w1]\r\n",
        "            v2 = model[w2]\r\n",
        "            pred.append(np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2)))\r\n",
        "\r\n",
        "    return spearmanr(sim, pred)\r\n",
        "\r\n",
        "def eval_bats_file(model, matrix, vocab, indices, f, repeat=False,\r\n",
        "                   multi=0):\r\n",
        "    '''\r\n",
        "        Evaluates a trained embedding model on a single BATS file using either\r\n",
        "        3CosAdd (the classic vector offset cosine method) or 3CosAvg (held-out\r\n",
        "        averaging).\r\n",
        "\r\n",
        "        If multi is set to zero or None, this function will usee 3CosAdd;\r\n",
        "        otherwise it will use 3CosAvg, holding out (multi) samples at a time.\r\n",
        "\r\n",
        "        Default behavior is to use 3CosAdd.\r\n",
        "    '''\r\n",
        "    pairs = [line.strip().split() for line in open(f, 'r').readlines()]\r\n",
        "\r\n",
        "    # discard pairs that are not in our vocabulary\r\n",
        "    pairs = [[p[0], p[1].split('/')] for p in pairs if p[0] in model]\r\n",
        "    pairs = [[p[0], [w for w in p[1] if w in model]] for p in pairs]\r\n",
        "    pairs = [p for p in pairs if len(p[1]) > 0]\r\n",
        "    if len(pairs) <= 1: return None\r\n",
        "\r\n",
        "    transposed = np.transpose(np.array([x / norm(x) for x in matrix]))\r\n",
        "\r\n",
        "    if not multi:\r\n",
        "        qa = []\r\n",
        "        qb = []\r\n",
        "        qc = []\r\n",
        "        targets = []\r\n",
        "        exclude = []\r\n",
        "        groups = []\r\n",
        "        \r\n",
        "        for i in range(len(pairs)):\r\n",
        "            j = random.randint(0, len(pairs) - 2)\r\n",
        "            if j >= i: j += 1\r\n",
        "            a = model[pairs[i][0]]\r\n",
        "            c = model[pairs[j][0]]\r\n",
        "            for bw in pairs[i][1]:\r\n",
        "                qa.append(a)\r\n",
        "                qb.append(model[bw])\r\n",
        "                qc.append(c)\r\n",
        "                groups.append(i)\r\n",
        "                targets.append(pairs[j][1])\r\n",
        "                exclude.append([pairs[i][0], bw, pairs[j][0]])\r\n",
        "\r\n",
        "        for queries in [qa, qb, qc]:\r\n",
        "            queries = np.array([x / norm(x) for x in queries])\r\n",
        "        \r\n",
        "        sa = np.matmul(qa, transposed) + .0001\r\n",
        "        sb = np.matmul(qb, transposed)\r\n",
        "        sc = np.matmul(qc, transposed)\r\n",
        "        sims = sb + sc - sa\r\n",
        "\r\n",
        "        # exclude original query words from candidates\r\n",
        "        for i in range(len(exclude)):\r\n",
        "            for w in exclude[i]:\r\n",
        "                sims[i][indices[w]] = 0\r\n",
        "\r\n",
        "    else:\r\n",
        "        offsets = []\r\n",
        "        exclude = []\r\n",
        "        preds = []\r\n",
        "        targets = []\r\n",
        "        groups = []\r\n",
        "        \r\n",
        "        for i in range(len(pairs) // multi):\r\n",
        "            qa = [pairs[j][0] for j in range(len(pairs)) if j - i not in range(multi)]\r\n",
        "            qb = [[w for w in pairs[j][1] if w in model] for j in range(len(pairs)) if j - i not in range(multi)]\r\n",
        "            qbs = []\r\n",
        "            for ws in qb: qbs += ws\r\n",
        "            a = np.mean([model[w] for w in qa], axis=0)\r\n",
        "            b = np.mean([np.mean([model[w] for w in ws], axis=0) for ws in qb], axis=0)\r\n",
        "            a = a / norm(a)\r\n",
        "            b = b / norm(b)\r\n",
        "\r\n",
        "            for k in range(multi):\r\n",
        "                c = model[pairs[i + k][0]]\r\n",
        "                c = c / norm(c)\r\n",
        "                offset = b + c - a\r\n",
        "                offsets.append(offset / norm(offset))\r\n",
        "                targets.append(pairs[i + k][1])\r\n",
        "                exclude.append(qa + qbs + [pairs[i + k][0]])\r\n",
        "                groups.append(len(groups))\r\n",
        "\r\n",
        "        print(np.shape(transposed))\r\n",
        "\r\n",
        "        sims = np.matmul(np.array(offsets), transposed)\r\n",
        "        print(np.shape(sims))\r\n",
        "        for i in range(len(exclude)):\r\n",
        "            for w in exclude[i]:\r\n",
        "                sims[i][indices[w]] = 0\r\n",
        "\r\n",
        "    preds = [vocab[np.argmax(x)] for x in sims]\r\n",
        "    accs = [1 if preds[i].lower() in targets[i] else 0 for i in range(len(preds))]\r\n",
        "    regrouped = np.zeros(np.max(groups) + 1)\r\n",
        "    for a, g in zip(accs, groups):\r\n",
        "        regrouped[g] = max(a, regrouped[g])\r\n",
        "    return np.mean(regrouped)\r\n",
        "\r\n",
        "def eval_bats(model, matrix, vocab, indices):\r\n",
        "    '''\r\n",
        "        Evaluates a trained embedding model on BATS.\r\n",
        "\r\n",
        "        Returns a dictionary containing\r\n",
        "        { category : accuracy score over the category }, where \"category\" can\r\n",
        "        be\r\n",
        "            - any of the low-level category names (i.e. the prefix of any of\r\n",
        "              the individual data files)\r\n",
        "            - one of the four top-level categories (\"inflectional_morphology\",\r\n",
        "              \"derivational_morphology\", \"encyclopedic_semantics\",\r\n",
        "              \"lexicographic_semantics\")\r\n",
        "            - \"total\", for the overall score on the entire corpus\r\n",
        "    '''\r\n",
        "    accs = {}\r\n",
        "    base = 'data/BATS'\r\n",
        "    for dr in os.listdir('data/BATS'):\r\n",
        "        if os.path.isdir(os.path.join(base, dr)):\r\n",
        "            dk = dr.split('_', 1)[1].lower()\r\n",
        "            accs[dk] = []\r\n",
        "            for f in os.listdir(os.path.join(base, dr)):\r\n",
        "                accs[f.split('.')[0]] = eval_bats_file(model, matrix, vocab, indices, os.path.join(base, dr, f))\r\n",
        "                accs[dk].append(accs[f.split('.')[0]])\r\n",
        "            accs[dk] = [a for a in accs[dk] if a is not None]\r\n",
        "            accs[dk] = np.mean(accs[dk]) if len(accs[dk]) > 0 else None\r\n",
        "\r\n",
        "    accs['total'] = np.mean([accs[k] for k in accs.keys() if accs[k] is not None])\r\n",
        "\r\n",
        "    return accs\r\n",
        "\r\n",
        "def eval_msr(model):\r\n",
        "    '''\r\n",
        "        Evaluates a trained embedding model on the MSR paraphrase task using\r\n",
        "        logistic regression over cosine similarity scores.\r\n",
        "    '''\r\n",
        "    X_tr, y_tr = load_msr('data/msr/msr_paraphrase_train.txt')\r\n",
        "    X_test, y_test = load_msr('data/msr/msr_paraphrase_test.txt')\r\n",
        "\r\n",
        "    train = [[np.sum([model[w] for w in ss[0] if w in model], axis=0), np.sum([model[w] for w in ss[1] if w in model], axis=0)] for ss in X_tr]\r\n",
        "    test = [[np.sum([model[w] for w in ss[0] if w in model], axis=0), np.sum([model[w] for w in ss[1] if w in model], axis=0)] for ss in X_test]\r\n",
        "\r\n",
        "    tr_cos = np.array([1 - cosine(x[0], x[1]) for x in train]).reshape(-1, 1)\r\n",
        "    test_cos = np.array([1 - cosine(x[0], x[1]) for x in test]).reshape(-1, 1)\r\n",
        "\r\n",
        "    lr = LogisticRegression(class_weight='balanced', solver='liblinear')\r\n",
        "    lr.fit(tr_cos, y_tr)\r\n",
        "    preds = lr.predict(test_cos)\r\n",
        "\r\n",
        "    return accuracy_score(y_test, preds)\r\n",
        "\r\n",
        "def evaluate_models(files, verbose=True):\r\n",
        "    '''\r\n",
        "        Evaluates multiple models at a time. Returns results in a list where\r\n",
        "        each item is a dict containing\r\n",
        "        { \"wordsim\" : WordSim353 correlation,\r\n",
        "          \"bats\" : a dictionary of BATS scores (see eval_bats() for details),\r\n",
        "          \"msr\" : MSR paraphrase performance }.\r\n",
        "    '''\r\n",
        "    results = []\r\n",
        "\r\n",
        "    for f in files:\r\n",
        "        if verbose: print('[evaluate_models] Reading ' + f)\r\n",
        "        model = load_model(f)\r\n",
        "        matrix, vocab, indices = collect(model)\r\n",
        "        r = {}\r\n",
        "        if verbose: print('[evaluate_models] Evaluating on WordSim...')\r\n",
        "        r['wordsim'] = eval_wordsim(model)\r\n",
        "        if verbose: print('[evaluate_models] Evaluating on BATS...')\r\n",
        "        r['bats'] = eval_bats(model, matrix, vocab, indices)\r\n",
        "        if verbose: print('[evaluate_models] Evaluating on MSRPC...')\r\n",
        "        r['msr'] = eval_msr(model)\r\n",
        "        results.append(r)\r\n",
        "\r\n",
        "    return results\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "  \r\n",
        "\r\n",
        "    print('[evaluate] Loading model...')\r\n",
        "    model = load_model(\"/content/drive/My Drive/Colab Notebooks/data/SVD_10_1000.txt\")\r\n",
        "\r\n",
        "    print('[evaluate] Collecting matrix...')\r\n",
        "    matrix, vocab, indices = collect(model)\r\n",
        "\r\n",
        "    print('[evaluate] WordSim353 correlation:')\r\n",
        "    ws = eval_wordsim(model)\r\n",
        "    print(ws)\r\n",
        "\r\n",
        "    print('[evaluate] BATS accuracies:')\r\n",
        "    bats = eval_bats(model, matrix, vocab, indices)\r\n",
        "    print(bats)\r\n",
        "\r\n",
        "    print('[evaluate] MSR accuracy:')\r\n",
        "    msr = eval_msr(model)\r\n",
        "    print(msr)\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[evaluate] Loading model...\n",
            "[evaluate] Collecting matrix...\n",
            "[evaluate] WordSim353 correlation:\n",
            "SpearmanrResult(correlation=0.31814868774069544, pvalue=4.965725087469434e-09)\n",
            "[evaluate] BATS accuracies:\n",
            "{'inflectional_morphology': 0.004124149659863945, 'I06 [verb_inf - Ving]': 0.0, 'I01 [noun - plural_reg]': 0.02040816326530612, 'I04 [adj - superlative]': 0.0, 'I07 [verb_inf - Ved]': 0.0, 'I02 [noun - plural_irreg]': 0.020833333333333332, 'I05 [verb_inf - 3pSg]': 0.0, 'I10 [verb_3pSg - Ved]': 0.0, 'I08 [verb_Ving - 3pSg]': 0.0, 'I09 [verb_Ving - Ved]': 0.0, 'I03 [adj - comparative]': 0.0, 'derivational_morphology': 0.0, 'D01 [noun+less_reg]': 0.0, 'D03 [adj+ly_reg]': 0.0, 'D09 [verb+tion_irreg]': 0.0, 'D04 [over+adj_reg]': 0.0, 'D05 [adj+ness_reg]': 0.0, 'D02 [un+adj_reg]': 0.0, 'D06 [re+verb_reg]': 0.0, 'D10 [verb+ment_irreg]': 0.0, 'D08 [verb+er_irreg]': 0.0, 'D07 [verb+able_reg]': 0.0, 'encyclopedic_semantics': 0.0, 'E01 [country - capital]': 0.0, 'E02 [country - language]': 0.0, 'E06 [animal - young]': 0.0, 'E04 [name - nationality]': 0.0, 'E08 [animal - shelter]': 0.0, 'E07 [animal - sound]': 0.0, 'E05 [name - occupation]': 0.0, 'E03 [UK_city - county]': 0.0, 'E10 [male - female]': 0.0, 'E09 [things - color]': 0.0, 'lexicographic_semantics': 0.009507844401461423, 'L09 [antonyms - gradable]': 0.0, 'L02 [hypernyms - misc]': 0.0, 'L04 [meronyms - substance]': 0.0, 'L10 [antonyms - binary]': 0.022222222222222223, 'L06 [meronyms - part]': 0.0, 'L08 [synonyms - exact]': 0.0, 'L03 [hyponyms - misc]': 0.0425531914893617, 'L05 [meronyms - member]': 0.0, 'L07 [synonyms - intensity]': 0.0, 'L01 [hypernyms - animals]': 0.030303030303030304, 'total': 0.003407998515331342}\n",
            "[evaluate] MSR accuracy:\n",
            "0.6631884057971015\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}